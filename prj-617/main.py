import functions_framework
from google.cloud import bigquery, storage
import os
import datetime
import pandas as pd
import sys
from io import BytesIO
import logging


logging.basicConfig(level=logging.INFO)

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
PROJECT_ID = os.environ.get('PROJECT_ID')
DATASET_ID = os.environ.get('DATASET_ID')
TABLE_ID = os.environ.get('TABLE_ID')
BUCKET_NAME = os.environ.get('BUCKET_NAME')
GCS_PATH_SUFFIX = os.environ.get('GCS_PATH_SUFFIX', '')


def export_bq_to_gcs():
    """
    í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§: BQ -> Pandas -> GCS
    """
    # í™˜ê²½ ë³€ìˆ˜ ê²€ì¦
    if not all([PROJECT_ID, DATASET_ID, TABLE_ID, BUCKET_NAME]):
        raise ValueError('Missing required environment variables')

    # 1. íŒŒì¼ëª… ë° ê²½ë¡œ ìƒì„±
    datestamp = datetime.datetime.now().strftime("%Y%m%d")
    base_filename = f"{datestamp}_inner_page_source_data"
    csv_filename = f"{base_filename}.csv"
    excel_filename = f"{base_filename}.xlsx"
    blob_path = f"{GCS_PATH_SUFFIX.strip('/')}/" if GCS_PATH_SUFFIX else ""
    
    # 2. BigQueryì—ì„œ ë°ì´í„° ì½ê¸°
    table_path = f"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}"
    logging.info(f"ğŸ”— Reading from BigQuery: {table_path}")
    
    # read_gbqëŠ” ë°ì´í„°ê°€ í´ ê²½ìš° ë©”ëª¨ë¦¬ ì—ëŸ¬ë¥¼ ìœ ë°œí•  ìˆ˜ ìˆìœ¼ë‹ˆ ì£¼ì˜
    df = pd.read_gbq(f"SELECT * FROM `{table_path}`", project_id=PROJECT_ID)
    logging.info(f"ğŸ“Š Loaded {len(df):,} rows.")

    # 3. CSV ë³€í™˜
    csv_data = df.to_csv(index=False, encoding='utf-8-sig')
    
    # 4. Excel ë³€í™˜
    excel_buffer = BytesIO()
    df.to_excel(excel_buffer, index=False, engine='openpyxl')
    excel_data = excel_buffer.getvalue()

    # 5. GCS í´ë¼ì´ì–¸íŠ¸ ë° ë²„í‚· ì´ˆê¸°í™”
    storage_client = storage.Client()
    bucket = storage_client.bucket(BUCKET_NAME)
    
    # 6. í•´ë‹¹ ê²½ë¡œì˜ ê¸°ì¡´ CSV, Excel íŒŒì¼ ëª¨ë‘ ì‚­ì œ
    if blob_path:
        logging.info(f"ğŸ” Searching for existing CSV/Excel files in path: {blob_path}")
        prefix = blob_path.rstrip('/')  # ë§ˆì§€ë§‰ ìŠ¬ë˜ì‹œ ì œê±°í•˜ì—¬ prefixë¡œ ì‚¬ìš©
        
        deleted_count = 0
        for blob in bucket.list_blobs(prefix=prefix + '/'):
            blob_name = blob.name
            # ì •í™•íˆ í•´ë‹¹ ê²½ë¡œì˜ íŒŒì¼ë§Œ ì²˜ë¦¬ (í•˜ìœ„ ë””ë ‰í† ë¦¬ ì œì™¸)
            # blob_pathë¡œ ì‹œì‘í•˜ê³ , ê·¸ ì´í›„ì— ì¶”ê°€ ê²½ë¡œ êµ¬ë¶„ìê°€ ì—†ëŠ” ê²½ìš°ë§Œ
            if blob_name.startswith(blob_path):
                remaining_path = blob_name[len(blob_path):]
                # í•˜ìœ„ ë””ë ‰í† ë¦¬ê°€ ì•„ë‹Œ ê²½ìš° (ì¦‰, ë°”ë¡œ íŒŒì¼ì¸ ê²½ìš°)
                if '/' not in remaining_path:
                    if blob_name.endswith('.csv') or blob_name.endswith('.xlsx'):
                        logging.info(f"ğŸ—‘ï¸  Deleting: {blob_name}")
                        blob.delete()
                        deleted_count += 1
        
        if deleted_count > 0:
            logging.info(f"âœ… Deleted {deleted_count} existing file(s) from path: {blob_path}")
        else:
            logging.info(f"â„¹ï¸  No existing files found in path: {blob_path}")
    
    # 7. CSV ì—…ë¡œë“œ
    csv_blob_path = blob_path + csv_filename
    csv_blob = bucket.blob(csv_blob_path)
    csv_blob.upload_from_string(csv_data, content_type='text/csv; charset=utf-8')
    logging.info(f"âœ… Uploaded CSV file: {csv_blob_path}")
    
    # 8. Excel ì—…ë¡œë“œ
    excel_blob_path = blob_path + excel_filename
    excel_blob = bucket.blob(excel_blob_path)
    excel_blob.upload_from_string(excel_data, content_type='application/vnd.ms-excel')
    logging.info(f"âœ… Uploaded Excel file: {excel_blob_path}")


    return {"csv": csv_blob.name, "excel": excel_blob.name}


@functions_framework.http
def main_handler(request):
    """
    ì§„ì…ì (Entry Point) - ë°°í¬ ì‹œ ì´ í•¨ìˆ˜ ì´ë¦„ì„ 'ì§„ì…ì 'ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”.
    """
    # OPTIONS ìš”ì²­ ì²˜ë¦¬ (CORS)
    if request.method == 'OPTIONS':
        return ('', 204, {'Access-Control-Allow-Origin': '*'})

    try:
        # ë¡œì§ ì‹¤í–‰
        files = export_bq_to_gcs()
        
        logging.info("âœ… Pipeline finished successfully.")
        return {
            "status": "success",
            "files": files
        }, 200

    except Exception as e:
        # ì—ëŸ¬ ë°œìƒ ì‹œ ìƒì„¸ ì •ë³´ ë¡œê¹…
        error_msg = f"âŒ Pipeline failed: {str(e)}"
        logging.error(error_msg, exc_info=True) # exc_infoëŠ” Traceback ì „ì²´ë¥¼ ë¡œê¹…í•©ë‹ˆë‹¤.
        
        # Schedulerê°€ ì‹¤íŒ¨ë¡œ ì¸ì‹í•˜ë„ë¡ 500 ë¦¬í„´
        return {
            "status": "failed",
            "error": str(e)
        }, 500